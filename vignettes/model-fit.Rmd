---
title: "Estimating density and capture probability from electrofishing data"
author: "Colin Millar"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

```{r init, echo=FALSE}
knitr::opts_chunk$set(
  cache.path = "cache/model-fit/",
  fig.path = "figure/model-fit/",
  tidy = FALSE,
  comment="",
  message = FALSE,
  eval=TRUE,
  cache=FALSE)

library(sp)
require(spdep)
library(rstan)
```

# Estimating density and capture probability from electrofishing data

## Getting the data ready

In order to model the data we need to convert it to a more convient form and add in covariates.

```{r the-data}
# subset data to remove Stocking, low Runs and no area.
ef <- subset(CLdata::ef, !is.na(Site_OBJECTID) & !is.na(Area) & Runs > 2 & Stocked == "No")

# restructure ef
ef <- do.call(rbind, 
        lapply(c("S0", "SP", "T0", "TP"), 
            function(x) {
              out <- ef[c("Site_OBJECTID", "Site.Name", "Dataset", "Date", "Runs", "Area", "Trust", paste0(x, "_R", 1:6))]
              names(out) <- gsub(x, "n", names(out))
              out $ Species <- if (substring(x, 1, 1) == "S") "Salmon" else "Trout"
              out $ LifeStage <- if (substring(x, 2, 2) == "0") "Fry" else "Parr"
              out
            }
        ))

# drop rows with all NAs
ef <- ef[apply(ef[paste0("n_R", 1:6)], 1, function(x) !all(is.na(x))), ]

# tag on HMA data
hma <- CLdata::hma
hma <- hma[!(hma $ HAName %in% c("Shetlands", "Orkneys")),]
hma $ hmidx <- 1:nrow(hma)
gis <- CLdata::gis
gis @ data <- cbind(gis @ data, over(gis, hma))

# work out neighbourhood structure of hma
hmaadj0 <- poly2nb(hma, queen = FALSE)
hmaadj <- unclass(hmaadj0)
attributes(hmaadj) <- NULL
names(hmaadj) <- paste(1:length(hmaadj))
# connect hebrides
hmaadj[[which(hma $ HAName == "Outer Hebrides")]] <- which(hma $ HAName == "Inner Hebrides")
hmaadj[[which(hma $ HAName == "Inner Hebrides")]] <-  c(21, 42, 43)


# tag on gis data
ef <- cbind(ef, gis[ef $ Site_OBJECTID,])
# subset put sites above barriers
ef <- subset(ef, !barrier)
# fix missing value
ef $ n_R3[is.na(ef $ n_R3) & ef $ Runs == 3] <- 0
# add on data summaries
ef <- cbind(ef, CLmodel::getData(ef, passnames = paste0("n_R", 1:6)))
# add in some dates
ef $ pDate <- as.POSIXlt(ef $ Date, tz = "GMT", format = "%d/%m/%Y")
# try using lubridate
ef $ year <- ef $ pDate $ year + 1900
ef $ doy <- ef $ pDate $ yday
# add river classification
ef $ RIVCLASS <- sapply(strsplit(paste(ef $ RIVCODE), "/"), "[[", 2)


# trim data
ef <- subset(ef, doy > 150 & doy < 325 & year >= 1997 & year <= 2013)
# work only with salmon fry
#ef <- subset(ef, Species == "Salmon" & LifeStage == "Fry")
```

now the data is in a suitable form for modelling

```{r, dependson="the-data"}
str(ef)
```

## fitting a basic model: the basic likelihood

The likelihood of the site densities $\lambda$ and the capture probabilities $p$ can be written in terms of two statistics $T_i=\sum_{s} n_{is}$, the total number of fish caught in each electrofishing event, $i$, and $Z=$ a quantity which measures the effective number of fishing passes - i.e. whether all fish are caught during the first pass or whether they were spread out accross several fishing passes.

$L(\lambda, \beta; T, Z) = \prod_i (p_i (1-p_i)^{S_i - 1 - Z_i} \lambda_i)^{T_i} \exp(- (1-(1-p_i)^{S_i}) \lambda_i )$

We can model both the $p$ and the $\lambda$ parameters in terms of linear combinations of covariates by transforming the density and capture probability onto the real line:

$\log(\lambda_i) = A_i \alpha$ and $\log \frac{p_i}{1-p_i} = B_i \beta$

It is possible to estimate the $\alpha$ parameters conditional on the design matrix $B$ and then the estimates of $\beta$ can be estimated from a poisson regression.  However, to conduct model selection or to calculate confidence intervals the full likelihood must be used.

A most simple model is one where we assume the capture probabilities are everywhere constant.

Model fitting proceeds as follows.  First we estimate the capture probability using the optimiser in stan


```{r somefunctions, echo=FALSE}
# do some model selection
summaryMods <- function(lst, m0 = NULL) {
  aics <- sapply(lst, "[[", "aic")

  tab <- 
   data.frame(
    forms = sapply(lst, function(x) paste(deparse(x$formula, width.cutoff = 500L))),
    aic = aics
    )

  if (!is.null(m0)) tab $ Daic <- tab $ aic - AIC(m0)
  tab <- tab[order(aics),]

  unique(tab)  
}

getModels <- function(vars, n) {
  if (n > length(vars)) stop("n too big for number of variable")
  out <- do.call(expand.grid, lapply(1:n, function(i) 1:length(vars)))
  out <- unique(t(apply(out, 1, sort)))
  out <- out[apply(out, 1, function(x) !any(table(x)>1)),,drop=FALSE]
  if (n > 1) {
    apply(out, 1, function(x) paste(vars[x], collapse = " + "))
  } else {
    vars[out]
  }
}
```

we use the function `efp` which stands for electrofishing p to get the maximum likelihood estimates of $p$ given our model for $p$ and a full model for lambda, in this case it is one $p$ for everything.

```{r basicfit, dependson=c("the-data")}
m0 <- efp(X ~ 1, data = ef, passes = "Runs")
AIC(m0)
```

For brevity will focus on a few effects.  We know from preliminary analysis that space has a strong effect

```{r fit1, dependson=c("the-data")}
f1s <- c("Trust",
         "HAName",
         "DESCRIPTIO",
         "factor(CATCH_ID)",
         "poly(Water_W, 1)",
         "poly(Elevation_, 1)",
         "factor(year)",
         "s(year, k=3)",
         "poly(Area, 1)",
         "RIVCLASS"
         )
forms <- lapply(getModels(f1s, 1), function(x) as.formula(paste0("X ~ ", x)))
mods <- lapply(forms, efp, data = ef, passes = "Runs")
m0 <- efp(X ~ 1, data = ef, passes = "Runs")
summaryMods(mods, m0 = m0)
# summaries first fits

```




